{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of CASA0018-2-2-MNISTCNN.ipynb","provenance":[{"file_id":"https://github.com/djdunc/casa0018/blob/main/Week2/CASA0018_2_2_MNISTCNN.ipynb","timestamp":1610886851427}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qnyTxjK_GbOD"},"source":["# MNIST CNN\n","In this example we are still using the same MNIST dataset but this time we will be using Convolutions to try and improve the accuracy of our model.\n","\n","Information on the NMIST character dataset can be found [here](http://yann.lecun.com/exdb/mnist/).\n","\n","# Importing dependencies\n","Let's start by importing TensorFlow, printing out the version number and create an object that points to the MNIST data via the tf.keras datasets API. How did we know where the dataset was? TensorFlow Datasets provide a list of resources that are easily accessible in the Colabs - a catalogue of what is availabe is at: https://www.tensorflow.org/datasets/catalog/overview)\n"]},{"cell_type":"code","metadata":{"id":"q3KzJyjv3rnA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610882317389,"user_tz":0,"elapsed":2567,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"2d8041bf-77a6-4f3b-f579-b8245ce70384"},"source":["import tensorflow as tf\n","print(tf.__version__)\n","mnist = tf.keras.datasets.mnist"],"execution_count":2,"outputs":[{"output_type":"stream","text":["2.4.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GuoLQQBT4E-_"},"source":["\n","# Loading the image data\n","Calling **load_data** on the **mnist** object will give you two sets of lists, these will be the training and testing values for the graphics that contain the clothing items and their labels.\n"]},{"cell_type":"code","metadata":{"id":"BTdRgExe4TRB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610882317692,"user_tz":0,"elapsed":1405,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"4552ef37-7a9e-41f3-8cb6-bf009d1cc349"},"source":["(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rw395ROx4f5Q"},"source":["# Exploring the data and preparing it for use\n","What does these values look like? We are going to import a library that helps us display an image and point it to a training image, and a training label to see. Experiment with different indices in the array.\n"]},{"cell_type":"code","metadata":{"id":"FPc9d3gJ3jWF","colab":{"base_uri":"https://localhost:8080/","height":302},"executionInfo":{"status":"ok","timestamp":1610882328649,"user_tz":0,"elapsed":932,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"77dcdfe5-56be-4088-d2f0-7d8b96003149"},"source":["sample = 12;\n","print(\"shape:\", training_images[sample].shape)\n","\n","import matplotlib.pyplot as plt\n","plt.imshow(training_images[sample], cmap='gray')\n","print(\"Label:\", training_labels[sample])\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["shape: (28, 28)\n","Label: 3\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOPUlEQVR4nO3db6xU9Z3H8c93r8UYIYole72xsECjJs0aZUWyusZUTatrFMQHCCGK0Xj7oEob17hEH9RkU2PItsv6pMlFTemmS0OCBCyNRbHi+kDjBe8CwuWKBi03F+6qiaXxT4X73QdzaC4685t755wzZ+D7fiU3M3O+M/P7ZsKHc878ZuZn7i4AZ76/qboBAO1B2IEgCDsQBGEHgiDsQBBntXMwM+Otf6Bk7m71tufas5vZzWZ2wMwOmtmqPM8FoFzW6jy7mXVJGpL0PUmHJb0paZm770s8hj07ULIy9uwLJB109/fc/S+SfiNpUY7nA1CiPGG/SNIfx90+nG07hZn1mlm/mfXnGAtATqW/QefufZL6JA7jgSrl2bMPS5o57va3sm0AOlCesL8p6WIzm2NmUyQtlbSlmLYAFK3lw3h3P25mD0j6vaQuSc+6+9uFdQagUC1PvbU0GOfsQOlK+VANgNMHYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0G0vGRzp5k6dWqyfueddybrn3/+ebJ+5ZVXNqxNmzYt+djly5cn66+88kqyPjw8nKyX6ciRI8n65s2bk/X+/v4i20EOucJuZockHZN0QtJxd59fRFMAilfEnv16d/+wgOcBUCLO2YEg8obdJW0zs51m1lvvDmbWa2b9ZsbJG1ChvIfx17r7sJn9raQXzWzQ3V8dfwd375PUJ0lm5jnHA9CiXHt2dx/OLkclbZK0oIimABSv5bCb2blmNu3kdUnfl7S3qMYAFMvcWzuyNrO5qu3NpdrpwH+7+0+bPKa0w/jVq1cn6w8//HBZQ4c2NjaWrO/bt69hbf369cnHNqsfOnQoWY/K3a3e9pbP2d39PUmXt9wRgLZi6g0IgrADQRB2IAjCDgRB2IEgWp56a2mwEqfeDh48mKzPnTu3rKH10UcfJeu7d+8ubexmDhw4kKxfeumlyfr555+frM+bN2/SPU3Ubbfdlqxv3bq1tLFPZ42m3tizA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQZ8xPSd90003J+iWXXJKsDw0NtTz2p59+mqyPjIy0/NxVa/Yz2Xv27EnWZ82a1fLYCxcuTNaZZ58c9uxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EMQZM8/+7rvv5qqjvltvvTVZzzOP/sUXXyTra9eubfm58XXs2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgiDNmnh31TZkyJVl/6qmnkvW77767yHZOcfXVVyfrAwMDpY0dUdM9u5k9a2ajZrZ33LYLzOxFM3snu5xebpsA8prIYfwvJd38lW2rJG1394slbc9uA+hgTcPu7q9K+vgrmxdJWpddXyfp9oL7AlCwVs/Zu9395A+rHZHU3eiOZtYrqbfFcQAUJPcbdO7uqQUb3b1PUp9U7sKOANJanXo7amY9kpRdjhbXEoAytBr2LZJWZNdXSNpcTDsAytL0MN7M1kv6rqQZZnZY0k8kPSlpg5ndJ+l9SUvKbBJp119/fcPaXXfdlXzsPffck2vsL7/8MllfuXJlw9rg4GCusTE5TcPu7ssalG4suBcAJeLjskAQhB0IgrADQRB2IAjCDgTBV1xPAwsWLEjWt23b1rDW1dVVdDuncE9/KPKDDz5oWDtx4kTR7SCBPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME8+2lgyZL0N4jLnktPafZT1Vu3bm1Y6+/vTz72+eefT9Y3bdqUrO/duzdZj4Y9OxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EYc2+j1zoYKwI05JrrrkmWX/sscca1q666qrkY2fMmNFST51gbGwsWV+zZk3D2urVq5OPHR09fdc9cXert509OxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwTz7GW7WrFnJerN59u7u7mT9jjvuSNbvvffehjWzutPBbbFjx45k/cYb04sUN5vjr1LL8+xm9qyZjZrZ3nHbHjezYTMbyP5uKbJZAMWbyGH8LyXdXGf7f7j7Fdnf74ptC0DRmobd3V+V9HEbegFQojxv0D1gZruzw/zpje5kZr1m1m9m6R8cA1CqVsP+C0nflnSFpBFJP2t0R3fvc/f57j6/xbEAFKClsLv7UXc/4e5jktZKSi8zCqByLYXdzHrG3Vwsid/sBTpc03l2M1sv6buSZkg6Kukn2e0rJLmkQ5J+4O4jTQdjnj2c5cuXN6w9+OCDycc2W5e+TKtWrUrWm30fvkqN5tmbLhLh7svqbH4md0cA2oqPywJBEHYgCMIOBEHYgSAIOxAEX3FFZc46Kz0Z9NJLLyXr1113XZHtnOLpp59O1nt7e0sbOy9+ShoIjrADQRB2IAjCDgRB2IEgCDsQBGEHgmj6rTegLMePH0/Wd+7cmayXOc8+NDRU2nNXhT07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBPHsb9PT0JOv3339/sj44OJisb9iwYdI9dYKurq5k/fLLLy9t7GZz/K+//nppY1eFPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME8ewEuvPDCZP2FF15I1i+77LJkffr06ZPuqVN0d3c3rD300EPJx95www1Ft/NX+/fvT9Zfe+210sauStM9u5nNNLM/mNk+M3vbzH6Ubb/AzF40s3eyy9P3XyQQwEQO449L+hd3/46kf5T0QzP7jqRVkra7+8WStme3AXSopmF39xF335VdPyZpv6SLJC2StC672zpJt5fVJID8JnXObmazJc2T9IakbncfyUpHJNU9OTOzXkmduzAWEMSE3403s6mSNkr6sbv/aXzNa6tD1l200d373H2+u8/P1SmAXCYUdjP7hmpB/7W7P5dtPmpmPVm9R9JoOS0CKELTw3gzM0nPSNrv7j8fV9oiaYWkJ7PLzaV0eBpYs2ZNst5saq2ZOXPmJOsHDhxoWPvss89yjX3OOeck64888kiynppemzZtWks9nVT7p9nYsWPHGtZWrlyZa+zT0UTO2f9J0l2S9pjZQLbtUdVCvsHM7pP0vqQl5bQIoAhNw+7ur0lq9F/ojcW2A6AsfFwWCIKwA0EQdiAIwg4EQdiBIPiKawG2b9+erC9Zkm9WcteuXcn6W2+91bD2ySef5Br7vPPOS9bnzZuX6/nzSM2jS9LixYsb1nbs2FF0Ox2PPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBGG1H5lp02Bm7RusjWbPnp2sP/HEE8n60qVLC+zm9NFs2eRmvxOwcePGZP2NN96YdE9nAnev+y1V9uxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATz7G1w9tlnJ+up711LzZcuHhoaalhbuHBh8rHNDA4O5nr8yy+/3PJzDwwMJOuoj3l2IDjCDgRB2IEgCDsQBGEHgiDsQBCEHQii6Ty7mc2U9CtJ3ZJcUp+7/6eZPS7pfkn/l931UXf/XZPnCjnPDrRTo3n2iYS9R1KPu+8ys2mSdkq6XbX12P/s7v8+0SYIO1C+RmGfyPrsI5JGsuvHzGy/pIuKbQ9A2SZ1zm5msyXNk3Ty934eMLPdZvasmU1v8JheM+s3s/5cnQLIZcKfjTezqZJ2SPqpuz9nZt2SPlTtPP7fVDvUv7fJc3AYD5Ss5XN2STKzb0j6raTfu/vP69RnS/qtu/99k+ch7EDJWv4ijJmZpGck7R8f9OyNu5MWS9qbt0kA5ZnIu/HXSvofSXskjWWbH5W0TNIVqh3GH5L0g+zNvNRzsWcHSpbrML4ohB0oH99nB4Ij7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBNH0BycL9qGk98fdnpFt60Sd2lun9iXRW6uK7O3vGhXa+n32rw1u1u/u8ytrIKFTe+vUviR6a1W7euMwHgiCsANBVB32vorHT+nU3jq1L4neWtWW3io9ZwfQPlXv2QG0CWEHgqgk7GZ2s5kdMLODZraqih4aMbNDZrbHzAaqXp8uW0Nv1Mz2jtt2gZm9aGbvZJd119irqLfHzWw4e+0GzOyWinqbaWZ/MLN9Zva2mf0o217pa5foqy2vW9vP2c2sS9KQpO9JOizpTUnL3H1fWxtpwMwOSZrv7pV/AMPMrpP0Z0m/Orm0lpmtlvSxuz+Z/Uc53d3/tUN6e1yTXMa7pN4aLTN+jyp87Ypc/rwVVezZF0g66O7vuftfJP1G0qIK+uh47v6qpI+/snmRpHXZ9XWq/WNpuwa9dQR3H3H3Xdn1Y5JOLjNe6WuX6Kstqgj7RZL+OO72YXXWeu8uaZuZ7TSz3qqbqaN73DJbRyR1V9lMHU2X8W6nrywz3jGvXSvLn+fFG3Rfd627/4Okf5b0w+xwtSN57Rysk+ZOfyHp26qtATgi6WdVNpMtM75R0o/d/U/ja1W+dnX6asvrVkXYhyXNHHf7W9m2juDuw9nlqKRNqp12dJKjJ1fQzS5HK+7nr9z9qLufcPcxSWtV4WuXLTO+UdKv3f25bHPlr129vtr1ulUR9jclXWxmc8xsiqSlkrZU0MfXmNm52RsnMrNzJX1fnbcU9RZJK7LrKyRtrrCXU3TKMt6NlhlXxa9d5cufu3vb/yTdoto78u9KeqyKHhr0NVfS/2Z/b1fdm6T1qh3Wfanaexv3SfqmpO2S3pH0kqQLOqi3/1Jtae/dqgWrp6LerlXtEH23pIHs75aqX7tEX2153fi4LBAEb9ABQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBD/D8sadP72v5CEAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"3cbrdH225_nH"},"source":["Each of the 'pixel' values above are between 0 (black) and 255 (white). \n","\n","The first change needed from the previous example is the **input shape**. Previously each image was being passed into the input layer - ie a 28x28 pixel image with 1 colour depth. But now we need to pass in all the data as a single list - ie 60000 images - which means our input tensor is 60000x28x28x1 items in a list. This is also the same for the test data.\n","\n","We still '**normalize**' the data between 0 and 1 in the same way, ie in Python we just need to divide all values by 255.0:"]},{"cell_type":"code","metadata":{"id":"kRH19pWs6ZDn","executionInfo":{"status":"ok","timestamp":1610882584604,"user_tz":0,"elapsed":901,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["training_images=training_images.reshape(60000, 28, 28, 1)\n","training_images=training_images / 255.0\n","test_images = test_images.reshape(10000, 28, 28, 1)\n","test_images=test_images/255.0"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dIn7S9gf62ie"},"source":["# Building the model\n","To create the sequence of layers in a model we use  '**Sequential**'. The model has 7 layers with the first 4 being a mix of convlution and pooling. \n","\n","We use **Conv2d** to specify a convolution layer and assign **64 convolutions** of size **3x3**, we use a\n","'**relu**' (Rectified Linear Unit - ReLU) activation function which converts any negative value to zero (to keep us between 0 and 1) and define the input shape to be 28x28 pixels with 1 colour depth).\n","\n","In the second layer we use **MaxPooling** to define a pooling layer which compresses the image while maintaining features. By specifying a size **2x2** we are effectively quartering the size of the image. \n","\n","We then repeat the same pattern again for layers 3 and 4.\n","\n","The remaining layers are the same as the previous model architecture. Layer 5 is called '**Flatten**' which takes the previous layer and turns it into a 1 dimensional set. The 6th layer '**Dense**' has 128 neurons and also uses '**relu**' and the 7th layer '**Dense**' has 10 neurons which relate to the 10 MNIST categories. It implements a '**softmax**' activition function which looks at all the probabilities in that layer of neurons and sets the highest value to 1 and all the others to 0 - this makes it programatically easier to find the most likely solution.  \n","\n","Finally we output a model summary - note how many parameters this model creates compared to ones built so far."]},{"cell_type":"code","metadata":{"id":"7mAyndG3kVlK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610882594739,"user_tz":0,"elapsed":6328,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"9cd429c7-d820-4de4-a28b-c2ed68d75fd7"},"source":["model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","  tf.keras.layers.MaxPooling2D(2,2),\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","model.summary()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 64)        640       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 1600)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 128)               204928    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 10)                1290      \n","=================================================================\n","Total params: 243,786\n","Trainable params: 243,786\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c8vbMCqb9Mh6"},"source":["After defining the model we build it by compiling with an optimizer and loss function and then train it by calling **model.fit** to fit your training data to your training labels. "]},{"cell_type":"code","metadata":{"id":"BLMdl9aP8nQ0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610882742531,"user_tz":0,"elapsed":27614,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"13a02561-e6f8-4965-f732-45aabee638da"},"source":["model.compile(optimizer = tf.keras.optimizers.Adam(),\n","              loss = 'sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.fit(training_images, training_labels, epochs=5)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","1875/1875 [==============================] - 10s 2ms/step - loss: 0.2942 - accuracy: 0.9080\n","Epoch 2/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0431 - accuracy: 0.9866\n","Epoch 3/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0267 - accuracy: 0.9913\n","Epoch 4/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0190 - accuracy: 0.9938\n","Epoch 5/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0118 - accuracy: 0.9965\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f90000c96a0>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"-JJMsvSB-1UY"},"source":["The accuracy value at the end of the final epoch is about 0.99 or about 99% accurate in classifying the training data. I.E., it figured out a pattern match between the image and the labels that worked 99% of the time. Whilst each epoch takes longer to run, this model is more accurate than the previous without the convolutions.\n","\n","Next we can call '**model.evaluate**', and pass in the test data, and it will report back the loss for each. Let's give it a try:"]},{"cell_type":"code","metadata":{"id":"WzlqsEzX9s5P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610882762315,"user_tz":0,"elapsed":1194,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"910238b7-07b7-4fb1-f78e-a767d1fa79d7"},"source":["model.evaluate(test_images, test_labels)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["313/313 [==============================] - 1s 2ms/step - loss: 0.0299 - accuracy: 0.9913\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.02991202101111412, 0.9912999868392944]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"6tki-Aro_Uax"},"source":["For me, that returned a accuracy of about .9895, which means it was about 98.95% accurate. As expected it probably would not do as well with *unseen* data as it did with data it was trained on!  "]},{"cell_type":"markdown","metadata":{"id":"vMgOMlE0m0Zi"},"source":["# Extras\n","\n","If you want to dig deeper into finding out were errors or misclassifications are coming from take a look at **TensorBoard**. It provides a useful tool to check your model results and it makes really useful visualisations for sharing your results. \n","\n","The example below shows a quick way of \"seeing\" a confusion matrix of the results. \n"]},{"cell_type":"code","metadata":{"id":"xtgbnkKKdUXM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610882825553,"user_tz":0,"elapsed":897,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"9bc2b5f7-d411-47c5-ede8-8bb17c82251b"},"source":["test_predictions = model.predict_classes(test_images)\n","result = tf.math.confusion_matrix(test_labels, test_predictions)\n","print(result)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n","  warnings.warn('`model.predict_classes()` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["tf.Tensor(\n","[[ 979    0    0    0    0    0    0    1    0    0]\n"," [   0 1131    0    1    0    2    0    1    0    0]\n"," [   0    0 1023    0    0    0    1    5    3    0]\n"," [   0    0    1  997    0    9    0    1    2    0]\n"," [   0    0    0    0  978    0    3    1    0    0]\n"," [   1    0    0    4    0  885    0    1    0    1]\n"," [   1    3    1    0    1    1  950    0    1    0]\n"," [   0    3    2    0    0    1    0 1018    2    2]\n"," [   3    0    1    0    0    1    0    1  966    2]\n"," [   0    1    0    0    9    5    0    4    4  986]], shape=(10, 10), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JRf6pFddoXJ6"},"source":["To see how you can visualise this in TensorBoard take a look at these two links:\n","\n","\n","1.   [Intro to TensorBoard](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/get_started.ipynb#scrollTo=-Iue509kgOyE)\n","2.   [Displaying image data in TensorBoard](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/image_summaries.ipynb)\n"]}]}